{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b217319-a255-467d-82de-0539d1a9e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bae1f5-7954-4f74-924b-47359e55d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Input.xlsx'\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6bf782-2331-4b6a-ae2a-420b493e8657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URL_ID                                                URL\n",
       "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       "3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       "4  bctech2015  https://insights.blackcoffer.com/streamlined-t..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b0ca6-e4cb-4580-996e-1e0e4bbd4ed8",
   "metadata": {},
   "source": [
    "#### Extracting articles from the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d9c213-dbf3-46d1-8b53-226e3cf23189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles have been successfully extracted and saved.\n",
      "Scrapping time: 2.94 seconds.\n"
     ]
    }
   ],
   "source": [
    "def extract_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        # Extract the title\n",
    "        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else 'No Title Found'\n",
    "        \n",
    "        # Extract the article text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = '\\n'.join([para.get_text(strip=True) for para in paragraphs])\n",
    "        \n",
    "        return f\"{title}\\n\\n{article_text}\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Process each URL and save the content\n",
    "for index, row in df.iterrows():\n",
    "    start_time = time.time()\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    article_content = extract_article(url)\n",
    "    \n",
    "    # Save the article content to a text file\n",
    "    with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(article_content)\n",
    "\n",
    "print(\"Articles have been successfully extracted and saved.\")\n",
    "time_difference = time.time() - start_time\n",
    "print(f'Scrapping time: %.2f seconds.' % time_difference) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8800de8-c2d7-401e-99a9-59775b17566b",
   "metadata": {},
   "source": [
    "#### Step 1: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ecd734-e813-4c47-8588-6a58edb16c17",
   "metadata": {},
   "source": [
    "##### 1.1 Cleaning using Stop Words lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0f416-9463-415d-9261-9739da1d86d6",
   "metadata": {},
   "source": [
    "Start by loading the stopwords and cleaning the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec74025-0d42-4961-981a-ce9f547fc2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_paths):\n",
    "    \"\"\"This function loads the list of Stop Words.\"\"\"\n",
    "    stopwords = set()\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            words = file.read().splitlines()\n",
    "            stopwords.update(word.strip().lower() for word in words if word.strip())\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce5007-956b-4212-a1e1-dbbcfd258090",
   "metadata": {},
   "source": [
    "##### 1.2 Modify the Text Cleaning Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7a148-89ec-4e70-9f8b-a719f33acf1d",
   "metadata": {},
   "source": [
    "Use the combined of custom stop words set for cleaning the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "78430514-e162-4e14-9644-d9a2730f1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    cleaned_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "def process_file(file_path, stop_words):\n",
    "    with open(file_path, 'r',encoding=\"utf8\") as file:\n",
    "        text = file.read()\n",
    "    cleaned_tokens = clean_text(text, stop_words)\n",
    "    return cleaned_tokens, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ca4aa-59fc-4b0b-ba29-aff0868cbf8e",
   "metadata": {},
   "source": [
    "##### 1.3 Creating a Dictionary of Positive and Negative Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b11bb6-7545-43f7-8a6a-f10264c8ad32",
   "metadata": {},
   "source": [
    "Load your positive and negative words dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4703c790-f9fc-4049-abd5-68f1900928ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionary(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.read().splitlines()\n",
    "    return set(words)\n",
    "positive_words = load_dictionary('MasterDictionary/positive-words.txt')\n",
    "negative_words = load_dictionary('MasterDictionary/negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74524141-b6b1-4586-939b-8353478ffc93",
   "metadata": {},
   "source": [
    "##### 1.4 Extracting Derived Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "447164ef-e95c-4844-ade6-e583ba5905aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_scores(tokens, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
    "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
    "\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
    "\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2975e-6242-439b-a131-84644b7daeaa",
   "metadata": {},
   "source": [
    "#### Step 2: Analysis of Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28b6bb-59b0-421f-a086-cec9275a7c07",
   "metadata": {},
   "source": [
    "Gunning Fog Index Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb67fc20-6afe-4bef-adab-6f9adcbdfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunning_fog_index(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    complex_words = [word for word in words if len([char for char in word if char.lower() in 'aeiou']) > 2]\n",
    "    \n",
    "    average_sentence_length = len(words) / len(sentences)\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    \n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    return fog_index  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac42a87-a7d6-4968-a10b-72cdfdc8a946",
   "metadata": {},
   "source": [
    "#### Step 3: Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67dcaa98-9e2b-4ca1-9432-43b762b87abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_words_per_sentence(tokens):\n",
    "    total_chars = sum(len(word) for word in tokens if word.isalpha())\n",
    "    return total_chars / len(tokens) if tokens else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4b33c-7efe-4103-b0e2-4f1faef06c81",
   "metadata": {},
   "source": [
    "#### Step 4: Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a14fadaf-7fae-467d-bf8c-c2c5d679d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_complex_words(tokens):\n",
    "    complex_words = [word for word in tokens if len([char for char in word if char.lower() in 'aeiou']) > 2]\n",
    "    return len(complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424425c3-50a0-4adb-86ee-5832ab49238f",
   "metadata": {},
   "source": [
    "#### Step 5: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06cca3df-1fb4-48e2-bb72-9dd286792d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text, stop_words):\n",
    "    tokens = clean_text(text, stop_words)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f3360-2aeb-4741-b494-d617ca72621b",
   "metadata": {},
   "source": [
    "#### Step 6: Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe31168a-4d66-44cf-bdfd-557a0132981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = [x.lower() for x in word]\n",
    "    syllables = len([char for char in word if char in 'aeiou'])\n",
    "    for x in word:\n",
    "        if x.endswith(('es', 'ed')):\n",
    "            syllables -= 1\n",
    "        return max(1, syllables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ab618-61ce-42b0-8e66-3212d2ef6750",
   "metadata": {},
   "source": [
    "#### Step 7: Personal Pronouns Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3d7b416-6d8e-46a2-958f-b595600174e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314ca9e-64be-4de7-b00a-686fe9b63ad7",
   "metadata": {},
   "source": [
    "#### Step 8: Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "737ae8e2-244d-43b2-9513-76d27b616cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(tokens):\n",
    "    total_chars = sum(len(word) for word in tokens if word.isalpha())\n",
    "    return total_chars / len(tokens) if tokens else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4230c-f6ad-4a8f-99b6-45020a601674",
   "metadata": {},
   "source": [
    "#### Step 9: Average length of Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8c28e64-5aa2-4d61-ae87-d7b49a158963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Split each sentence into words and count them\n",
    "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
    "    \n",
    "    # Calculate the average sentence length\n",
    "    if len(sentence_lengths) > 0:\n",
    "        avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    else:\n",
    "        avg_sentence_length = 0\n",
    "    \n",
    "    return avg_sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf6a7b-9a11-4adf-8989-852ec545dfd8",
   "metadata": {},
   "source": [
    "#### Step 10: Analysis of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0ea0d72-a033-49b6-b29a-0ad600f60da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_file(file_path, stop_words,text_id):\n",
    "    # Process the file and get cleaned tokens and original text\n",
    "    cleaned_tokens, original_text = process_file(file_path, stop_words)\n",
    "    \n",
    "    # Calculate sentiment analysis scores\n",
    "    positive_score, negative_score, polarity_score, subjectivity_score = calculate_sentiment_scores(cleaned_tokens, positive_words, negative_words)\n",
    "    \n",
    "    # Calculate readability metrics\n",
    "    fog_index = gunning_fog_index(original_text)\n",
    "    average_words = average_words_per_sentence(original_text)\n",
    "    complex_word_count = count_complex_words(cleaned_tokens)\n",
    "    syllable_per_word = syllable_count(cleaned_tokens)\n",
    "    avg_sent_len = average_sentence_length(original_text)\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    total_words = len(cleaned_tokens)\n",
    "    pronoun_count = count_personal_pronouns(original_text)\n",
    "    avg_word_length = average_word_length(cleaned_tokens)\n",
    "    perc_complex_words = complex_word_count/total_words\n",
    "    \n",
    "    # Create a dictionary with all the metrics\n",
    "    result = {\n",
    "        'Text ID': text_id,\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score,\n",
    "        'Avg Sentence Length':avg_sent_len,\n",
    "        'Percentage of Complex Words':perc_complex_words,\n",
    "        'Fog Index': fog_index,\n",
    "        'Avg Number Words per Sentence': average_words,\n",
    "        'Complex Word Count': complex_word_count,\n",
    "        'Word Count': total_words,\n",
    "        'Syllable per Word':syllable_per_word,\n",
    "        'Personal Pronouns': pronoun_count,\n",
    "        'Average Word Length': avg_word_length\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame to facilitate appending to Excel\n",
    "    result_df = pd.DataFrame([result])\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5265a74f-dfad-49fd-9223-a55ac4c9873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All analysis results have been saved to Updated_Output_final.xlsx\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    stopword_files = [\n",
    "    r'StopWords\\StopWords_Auditor.txt',\n",
    "    r'StopWords\\StopWords_Currencies.txt',\n",
    "    r'StopWords\\StopWords_DatesandNumbers.txt',\n",
    "    r'StopWords\\StopWords_Generic.txt',\n",
    "    r'StopWords\\StopWords_GenericLong.txt',\n",
    "    r'StopWords\\StopWords_Geographic.txt',\n",
    "    r'StopWords\\StopWords_Names.txt'\n",
    "    ]\n",
    "    custom_stopwords = load_stopwords(stopword_files)\n",
    "\n",
    "    # List of files to process\n",
    "    text_files = [\n",
    "        'bctech2011.txt','bctech2012.txt','bctech2013.txt','bctech2014.txt','bctech2015.txt','bctech2016.txt','bctech2017.txt','bctech2018.txt',\n",
    "        'bctech2019.txt','bctech2020.txt','bctech2021.txt','bctech2022.txt','bctech2023.txt','bctech2024.txt','bctech2025.txt','bctech2026.txt',\n",
    "        'bctech2027.txt','bctech2028.txt','bctech2029.txt','bctech2030.txt','bctech2031.txt','bctech2032.txt','bctech2033.txt','bctech2034.txt',\n",
    "        'bctech2035.txt','bctech2036.txt','bctech2037.txt','bctech2038.txt','bctech2039.txt','bctech2040.txt','bctech2041.txt','bctech2042.txt',\n",
    "        'bctech2043.txt','bctech2044.txt','bctech2045.txt','bctech2046.txt','bctech2047.txt','bctech2048.txt','bctech2049.txt','bctech2050.txt',\n",
    "        'bctech2051.txt','bctech2052.txt','bctech2053.txt','bctech2054.txt','bctech2055.txt','bctech2056.txt','bctech2057.txt','bctech2058.txt',\n",
    "        'bctech2059.txt','bctech2060.txt','bctech2061.txt','bctech2062.txt','bctech2063.txt','bctech2064.txt','bctech2065.txt','bctech2066.txt',\n",
    "        'bctech2067.txt','bctech2068.txt','bctech2069.txt','bctech2070.txt','bctech2071.txt','bctech2072.txt','bctech2073.txt','bctech2074.txt',\n",
    "        'bctech2075.txt','bctech2076.txt','bctech2077.txt','bctech2078.txt','bctech2079.txt','bctech2080.txt','bctech2081.txt','bctech2082.txt',\n",
    "        'bctech2083.txt','bctech2084.txt','bctech2085.txt','bctech2086.txt','bctech2087.txt','bctech2088.txt','bctech2089.txt','bctech2090.txt',\n",
    "        'bctech2091.txt','bctech2092.txt','bctech2093.txt','bctech2094.txt','bctech2095.txt','bctech2096.txt','bctech2097.txt','bctech2098.txt',\n",
    "        'bctech2099.txt','bctech2100.txt','bctech2101.txt','bctech2102.txt','bctech2103.txt','bctech2104.txt','bctech2105.txt','bctech2106.txt',\n",
    "        'bctech2107.txt','bctech2108.txt','bctech2109.txt','bctech2110.txt','bctech2111.txt','bctech2112.txt','bctech2113.txt','bctech2114.txt',\n",
    "        'bctech2115.txt','bctech2116.txt','bctech2117.txt','bctech2118.txt','bctech2119.txt','bctech2120.txt','bctech2121.txt','bctech2122.txt',\n",
    "        'bctech2123.txt','bctech2124.txt','bctech2125.txt','bctech2126.txt','bctech2127.txt','bctech2128.txt','bctech2129.txt','bctech2130.txt',\n",
    "        'bctech2131.txt','bctech2132.txt','bctech2133.txt','bctech2134.txt','bctech2135.txt','bctech2136.txt','bctech2137.txt','bctech2138.txt',\n",
    "        'bctech2139.txt','bctech2140.txt','bctech2141.txt','bctech2142.txt','bctech2143.txt','bctech2144.txt','bctech2145.txt','bctech2146.txt',\n",
    "        'bctech2147.txt','bctech2148.txt','bctech2149.txt','bctech2150.txt','bctech2151.txt','bctech2152.txt','bctech2153.txt','bctech2154.txt',\n",
    "        'bctech2155.txt','bctech2156.txt','bctech2157.txt']\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    all_results_df = pd.DataFrame()\n",
    "\n",
    "    # Process each file\n",
    "    for idx, file_path in enumerate(text_files, start=1):\n",
    "        result_df = analyze_file(file_path, custom_stopwords, text_id=idx)\n",
    "        all_results_df = pd.concat([all_results_df, result_df], ignore_index=True)\n",
    "\n",
    "    #Load the Outout file\n",
    "    output_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "    output_df = output_df.loc[:, ['URL_ID', 'URL']]\n",
    "\n",
    "    #Final df\n",
    "    final_df = pd.concat([output_df,all_results_df],axis=1)\n",
    "    final_df = final_df.drop('Text ID',axis=1)\n",
    "    \n",
    "\n",
    "    # Save all results to Excel\n",
    "    output_file_path = 'Updated_Output_final.xlsx'\n",
    "    with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:\n",
    "        final_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "\n",
    "    print(f\"All analysis results have been saved to {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813ec50-174b-4e0f-b841-5f20d81eb030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
